# Phase 2 (Glyph Classification) Configuration
# Aligned strictly with NEW_PLAN.md Sections 7.1–7.3
# Purpose: Classify full glyphs from their 16x16 grid of primitive IDs (0..1023).
# Pipeline assumption:
#   - Phase 1 has produced: grids/<glyph_id>.u16 (shape 16x16), primitive_centroids.npy, label_map.json
#   - Rasterization already applied scaling/diacritic heuristic; grids are row‑major, no cropping.
#
# Key Plan Points Incorporated:
#   - Primitive embedding: Embedding(1024, 64)
#   - 2D positional encoding (sinusoidal or learnable) added to embeddings
#   - Transformer: 4–6 layers; choose 5 (midpoint), d_model=256, heads=8, MLP=512, dropout=0.1
#   - Batch size: 128
#   - Optimizer: AdamW lr=5e-4
#   - Epochs: 30–50 → choose 40 baseline
#   - Regularization: dropout 0.1, label smoothing 0.05
#   - Optional auxiliary diacritic presence loss (disabled by default, weight=0.1 in plan suggestion)
#
# Omitted (not in plan): token masking, advanced sequence augmentations, extra special tokens.

experiment:
  name: phase2_transformer_baseline_v1
  notes: >
    Baseline lightweight transformer on primitive ID grids.
    Embedding(1024,64) + 2D positional enc + 5-layer encoder (d_model=256).
    Midpoint hyperparameters of specified plan ranges.

seed: 42
deterministic: true

# ------------------------------------------------------------------
# Data Paths
# ------------------------------------------------------------------
data:
  root: data/grids_memmap
  grids_dir: data/grids_memmap # Memmap root; single packed file instead of per-glyph .u16 files
  labels_file: data/grids_memmap/label_map.json
  index_train: data/grids_memmap/splits/phase2_train_ids.txt
  index_val: data/grids_memmap/splits/phase2_val_ids.txt
  index_test: data/grids_memmap/splits/phase2_test_ids.txt
  primitive_centroids: assets/centroids/primitive_centroids.npy # Can re-enable centroid init after validation
  # Memmap-specific files:
  memmap_grids_file: data/grids_memmap/grids_uint16.npy
  memmap_row_ids_file: data/grids_memmap/glyph_row_ids.npy
  glyph_labels_file: data/grids_memmap/glyph_labels.jsonl
  # Format expectations:
  #   - label_map.json: { "label_string": class_index, ... } stable sorted order from preprocessing
  #   - primitive_centroids: produced during K-Means (Section 6.1)

# ------------------------------------------------------------------
# Input Representation (Section 7.1)
# ------------------------------------------------------------------
input:
  grid_rows: 16
  grid_cols: 16
  primitive_vocab_size: 1024 # IDs 0..1023 (0 = EMPTY)
  embedding_dim: 64
  positional_encoding: sinusoidal_2d # {sinusoidal_2d | learnable_2d}
  combine_mode: add # How positional enc is merged with primitive embeddings
  patch_grouping:
    enabled: true # GPU: enable 4x4 grouping → reduces tokens (256 → 64) for any transformer use
    patch_rows: 4
    patch_cols: 4
  token_pooling: cls # {cls | mean}; plan allows global average OR CLS
  use_cls_token: true # Adds an extra learnable CLS (sequence len = 256+1)
  normalize_embeddings: false # L2 normalize embeddings (not in plan; default off)

# ------------------------------------------------------------------
# Model Definition (Section 7.2.A)
# ------------------------------------------------------------------
# model:
#   architecture: cnn
#   # architecture: transformer
#   cnn:
#     stages: [64, 128, 192] # or omit to auto-generate
#     blocks_per_stage: [2, 2, 2]
#     dropout: 0.1
#     activation: gelu
#     kernel_size: 3
#     stem_kernel_size: 3
#     stem_stride: 1
#     downsample: conv
#     classifier_hidden_dim: 0 # 0 => single linear
#     classifier_dropout: 0.1
model:
  architecture: cnn
  transformer:
    num_layers: 5 # Plan: 4–6 → picked midpoint
    d_model: 256
    num_heads: 8
    mlp_hidden_dim: 512
    dropout: 0.1
    attention_dropout: 0.1
    layer_norm_eps: 1.0e-5
    pre_norm: true
  classifier:
    hidden_dim: 256 # Single linear if hidden_dim == d_model; can add MLP later
    dropout: 0.1
    activation: gelu
  # CNN alternative architecture configuration (used when model.architecture: cnn)
  cnn:
    stages: [64, 128, 192]
    blocks_per_stage: [2, 2, 2]
    kernel_size: 3
    stem_kernel_size: 3
    stem_stride: 1
    downsample: conv
    activation: gelu
    dropout: 0.1
    classifier_hidden_dim: 0 # 0 => single linear head
    classifier_dropout: 0.2 # Increased from 0.1 (quick win regularization)
  init:
    embedding_from_centroids: false # Disabled for memmap run; enable after verifying centroids alignment
    centroid_requires_grad: true
    cls_init: normal
    weight_init: xavier_uniform

# ------------------------------------------------------------------
# Optimization & Training (Section 7.3)
# ------------------------------------------------------------------
optim:
  name: adamw
  lr: 0.0005 # Plan: 5e-4
  weight_decay: 1.0e-4
  betas: [0.9, 0.999]
  eps: 1.0e-8
  grad_clip_norm: 1.0

scheduler:
  strategy: plateau # {cosine | plateau}; plan allows Cosine or ReduceLROnPlateau

  plateau: # Used only if strategy == plateau
    metric: val/accuracy_top1
    mode: max
    factor: 0.5
    patience: 3
    min_lr: 1.0e-6

training:
  unweighted_finetune_after_epoch: 19
  # (REMINDER) At/after this epoch:
  #   - set loss.class_weights: null
  #   - remove class_weights_alpha
  #   - set token_id_dropout: 0
  #   - optionally reduce LR again if not yet decayed
  epochs: 40 # Plan: 30–50
  batch_size: 1024 # GPU: larger batch for higher throughput (adjust if memory constrained)
  eval_batch_size: 1024 # Match train batch for efficient evaluation
  num_workers: 2 # Reduced for Kaggle inode/worker limits
  pin_memory: true # Keep true for faster H2D transfers
  gradient_accumulation_steps: 1
  auto_lr_scale: true # Scale LR by (batch_size / base_batch_size_for_lr) if enabled
  base_batch_size_for_lr: 256 # Reference batch size for auto LR scaling
  target_effective_batch_size: 4096 # If > batch_size, gradient accumulation steps are auto-adjusted
  compile: true # Enable torch.compile (PyTorch 2.x) for graph-level optimization
  log_interval_steps: 25 # Reduce console overhead with large step counts
  token_id_dropout: 0.02 # Keep; you can lower to 0 for final fine-tune
  val_interval_epochs: 1
  save_every_epochs: 1
  mixed_precision: amp # {none | amp} - leverage GPU Tensor Cores
  cache_grids: true # Preload all grids into RAM (ensure memory headroom)
  resume_from: checkpoints/phase2/best.pt

early_stopping:
  enabled: true
  metric: val/accuracy_top1
  mode: max
  patience: 6
  min_delta: 0.0005

# ------------------------------------------------------------------
# Loss Configuration
# ------------------------------------------------------------------
loss:
  primary: cross_entropy
  label_smoothing: 0.05
  class_weights: auto_inverse_freq # enabled auto inverse frequency weights
  class_weights_alpha: 0.3
  auxiliary_diacritic:
    enabled: false # Optional auxiliary (Section 7.3 table, weight 0.1)
    weight: 0.1
    label_source: metadata # Where to read diacritic flag (metadata boolean)
  reduction: mean

# ------------------------------------------------------------------
# Metrics (Section 8.2 subset relevant to phase)
# ------------------------------------------------------------------
metrics:
  - accuracy_top1
  - accuracy_top5
  - macro_f1
  - per_class_accuracy
  - diacritic_subset_accuracy # accuracy restricted to glyphs is_diacritic=true
  - ligature_subset_accuracy # if ligatures flagged; else ignore gracefully

# ------------------------------------------------------------------
# Checkpointing
# ------------------------------------------------------------------
checkpoint:
  dir: checkpoints/phase2
  monitor: val/accuracy_top1
  mode: max
  save_top_k: 3
  save_last: true
  filename_pattern: "epoch{epoch:02d}-val{val_accuracy_top1:.4f}.pt"
  keep_every_n_epochs: 0

# ------------------------------------------------------------------
# Logging
# ------------------------------------------------------------------
logging:
  backend: tensorboard # {tensorboard | wandb | none}
  tensorboard_dir: logs/phase2
  run_name: "${experiment.name}"
  log_hparams: true
  wandb:
    enabled: false
    project: glyph-grid
    tags: ["phase2", "transformer", "baseline"]

# ------------------------------------------------------------------
# Evaluation / Analysis Hooks
# ------------------------------------------------------------------
evaluation:
  confusion_matrix: true
  topk: [1, 5]
  attention_heatmap:
    enabled: true
    sample_count: 50
  export_embeddings:
    enabled: false
    layer: penultimate
    path: artifacts/phase2_glyph_embeddings.npy

# ------------------------------------------------------------------
# Sanity & Validation
# ------------------------------------------------------------------
sanity:
  expect_grid_shape: [16, 16]
  primitive_vocab_size: 1024
  embedding_dim: 64
  d_model: 256
  mlp_hidden_dim: 512
  assert_cls_token: true

# ------------------------------------------------------------------
# Debug
# ------------------------------------------------------------------
debug:
  fast_dev_run: false
  limit_train_batches: null
  limit_val_batches: null
  overfit_batches: 0
  deterministic_hash_check: false
# ------------------------------------------------------------------
# Notes
# ------------------------------------------------------------------
# - To try patch grouping (4x4→64 tokens), set patch_grouping.enabled: true and
#   adjust use_cls_token if desired.
# - If overfitting early, raise dropout to 0.15 or add stochastic depth.
# - For larger experiments (d_model=320, num_layers=6) adjust batch size or use grad accumulation.
# - Auxiliary diacritic loss can be enabled once metadata includes robust diacritic subtype flags.
